{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import h5py\n",
    "    from numba import jit, cuda \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    import timeit\n",
    "\n",
    "    %store -r IMAGE_SIZE\n",
    "    import glob\n",
    "    import os\n",
    "\n",
    "    # for reading and displaying images\n",
    "    from skimage.io import imread\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    # for creating validation set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from skimage.color import rgb2gray, gray2rgb\n",
    "\n",
    "    # for evaluating the model\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # PyTorch libraries and modules\n",
    "    import torch\n",
    "    from torch.autograd import Variable\n",
    "    from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "    from torch.optim import Adam, SGD\n",
    "\n",
    "    import torchvision\n",
    "except Exception as error:\n",
    "    !pip install {str(error)[17:-1]}\n",
    "except:\n",
    "    print(\"Pacote n√£o encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesions = {0: \"Basal Cell Carcinoma\", \n",
    "    1: \"Lentigo\",\n",
    "    2: \"Malignant Melanoma\", \n",
    "    3: \"Melanocytic naevus\",\n",
    "    4: \"seborrhoeic keratosis\",\n",
    "    5: \"Wart\", \n",
    "    6: \"Actinic Keratosis\",\n",
    "    7: \"Squamous Cell Carcinoma\",\n",
    "    8: \"Intraepithelial Carcinoma\", \n",
    "    9: \"Pyogenic Granuloma\",\n",
    "    10: \"Haemangioma\",\n",
    "    11: \"Dermatofibroma\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                  id  label\n",
       "0  Wart_original_100.jpg_b1cf4712-7089-44ae-8b03-...      5\n",
       "1  Wart_original_37.jpg_4b8537e9-0e10-4865-a920-a...      5\n",
       "2  Wart_original_2.jpg_4df8344c-e516-4ed5-a5b5-dc...      5\n",
       "3  Wart_original_21.jpg_3c281b36-dfec-46eb-abb8-1...      5\n",
       "4  Wart_original_54.jpg_294c8aac-a465-4a29-9030-7...      5"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wart_original_100.jpg_b1cf4712-7089-44ae-8b03-...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Wart_original_37.jpg_4b8537e9-0e10-4865-a920-a...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Wart_original_2.jpg_4df8344c-e516-4ed5-a5b5-dc...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Wart_original_21.jpg_3c281b36-dfec-46eb-abb8-1...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wart_original_54.jpg_294c8aac-a465-4a29-9030-7...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv',index_col=0)\n",
    "test = pd.read_csv('test.csv',index_col=0)\n",
    "val = pd.read_csv('val.csv',index_col=0)\n",
    "\n",
    "train_y = train['label'].values\n",
    "test_y = test['label'].values\n",
    "val_y = val['label'].values\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train = pd.DataFrame(train)\n",
    "print(df_train['label'].values.tolist()[50000])\n",
    "df_test = pd.DataFrame(test)\n",
    "df_val = pd.DataFrame(val)\n",
    "\n",
    "def img_array(dataframe, img_type):\n",
    "    train_img = []\n",
    "    for index, row in tqdm(dataframe.iterrows()):\n",
    "        # defining the image path\n",
    "        try:\n",
    "            if int(row['id']) or row['id'] == str(0):\n",
    "                image_path = 'dataset-split/' + img_type + '/' + lesions[row['label']] + \"/\" + str(row['id']) + '.jpg'\n",
    "        except:\n",
    "            image_path = 'dataset-split/' + img_type + '/' + lesions[row['label']] + \"/resnet_augmented/\" + str(row['id']) + '.jpg'\n",
    "        # reading the image\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "        # normalize\n",
    "        # img = cv2.normalize(img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "        # img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "        # appending the image into the list\n",
    "        train_img.append(img)\n",
    "\n",
    "    return train_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "209it [00:00, 2087.91it/s]image size: 4096 bytes\n",
      "1794it [00:00, 2322.42it/s]\n",
      "uint8\n",
      "hdf5 file size: 270049280 bytes\n"
     ]
    }
   ],
   "source": [
    "save_path = './train.hdf5'\n",
    "print('image size: %d bytes'%os.path.getsize('dataset-split/train'))\n",
    "hf = h5py.File(save_path, 'a') # open a hdf5 file\n",
    "\n",
    "binary_data_np = np.asarray(img_array(df_train, 'train'))\n",
    "dset = hf.create_dataset('train', data=binary_data_np)  # write the data to hdf5 file\n",
    "hf.close()  # close the hdf5 file\n",
    "print('hdf5 file size: %d bytes'%os.path.getsize(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "235it [00:00, 2247.64it/s]image size: 4096 bytes\n",
      "hdf5 file size: 35376128 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_path = './test.hdf5'\n",
    "print('image size: %d bytes'%os.path.getsize('dataset-split/test'))\n",
    "hf = h5py.File(save_path, 'a') # open a hdf5 file\n",
    "\n",
    "oi = img_array(df_test, 'test')\n",
    "\n",
    "binary_data_np = np.asarray(oi)\n",
    "dset = hf.create_dataset('test', data=binary_data_np)  # write the data to hdf5 file\n",
    "hf.close()  # close the hdf5 file\n",
    "print('hdf5 file size: %d bytes'%os.path.getsize(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"./train.hdf5\", \"r\") as hf:\n",
    "    x_train = hf['train'][:]\n",
    "with h5py.File(\"./test.hdf5\", \"r\") as hf:\n",
    "    x_val = hf['test'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}