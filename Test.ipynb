{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PyTorch Version:  1.7.1+cu110\nTorchvision Version:  0.8.2+cu110\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import numpy as np\n",
    "    import torchvision\n",
    "    import matplotlib.pyplot as plt\n",
    "    import time\n",
    "    import os\n",
    "    import copy\n",
    "    from torchvision import datasets, models, transforms\n",
    "    print(\"PyTorch Version: \",torch.__version__)\n",
    "    print(\"Torchvision Version: \",torchvision.__version__)\n",
    "except Exception as error:\n",
    "    !pip install {str(error)[17:-1]}\n",
    "except:\n",
    "    print(\"Pacote não encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 14]           2,048\n            ReLU-209         [-1, 1024, 14, 14]               0\n      Bottleneck-210         [-1, 1024, 14, 14]               0\n          Conv2d-211          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-212          [-1, 256, 14, 14]             512\n            ReLU-213          [-1, 256, 14, 14]               0\n          Conv2d-214          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-215          [-1, 256, 14, 14]             512\n            ReLU-216          [-1, 256, 14, 14]               0\n          Conv2d-217         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n            ReLU-219         [-1, 1024, 14, 14]               0\n      Bottleneck-220         [-1, 1024, 14, 14]               0\n          Conv2d-221          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-222          [-1, 256, 14, 14]             512\n            ReLU-223          [-1, 256, 14, 14]               0\n          Conv2d-224          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-225          [-1, 256, 14, 14]             512\n            ReLU-226          [-1, 256, 14, 14]               0\n          Conv2d-227         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n            ReLU-229         [-1, 1024, 14, 14]               0\n      Bottleneck-230         [-1, 1024, 14, 14]               0\n          Conv2d-231          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-232          [-1, 256, 14, 14]             512\n            ReLU-233          [-1, 256, 14, 14]               0\n          Conv2d-234          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-235          [-1, 256, 14, 14]             512\n            ReLU-236          [-1, 256, 14, 14]               0\n          Conv2d-237         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n            ReLU-239         [-1, 1024, 14, 14]               0\n      Bottleneck-240         [-1, 1024, 14, 14]               0\n          Conv2d-241          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-242          [-1, 256, 14, 14]             512\n            ReLU-243          [-1, 256, 14, 14]               0\n          Conv2d-244          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-245          [-1, 256, 14, 14]             512\n            ReLU-246          [-1, 256, 14, 14]               0\n          Conv2d-247         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n            ReLU-249         [-1, 1024, 14, 14]               0\n      Bottleneck-250         [-1, 1024, 14, 14]               0\n          Conv2d-251          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-252          [-1, 256, 14, 14]             512\n            ReLU-253          [-1, 256, 14, 14]               0\n          Conv2d-254          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-255          [-1, 256, 14, 14]             512\n            ReLU-256          [-1, 256, 14, 14]               0\n          Conv2d-257         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n            ReLU-259         [-1, 1024, 14, 14]               0\n      Bottleneck-260         [-1, 1024, 14, 14]               0\n          Conv2d-261          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-262          [-1, 256, 14, 14]             512\n            ReLU-263          [-1, 256, 14, 14]               0\n          Conv2d-264          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-265          [-1, 256, 14, 14]             512\n            ReLU-266          [-1, 256, 14, 14]               0\n          Conv2d-267         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n            ReLU-269         [-1, 1024, 14, 14]               0\n      Bottleneck-270         [-1, 1024, 14, 14]               0\n          Conv2d-271          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-272          [-1, 256, 14, 14]             512\n            ReLU-273          [-1, 256, 14, 14]               0\n          Conv2d-274          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-275          [-1, 256, 14, 14]             512\n            ReLU-276          [-1, 256, 14, 14]               0\n          Conv2d-277         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n            ReLU-279         [-1, 1024, 14, 14]               0\n      Bottleneck-280         [-1, 1024, 14, 14]               0\n          Conv2d-281          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-282          [-1, 256, 14, 14]             512\n            ReLU-283          [-1, 256, 14, 14]               0\n          Conv2d-284          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-285          [-1, 256, 14, 14]             512\n            ReLU-286          [-1, 256, 14, 14]               0\n          Conv2d-287         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n            ReLU-289         [-1, 1024, 14, 14]               0\n      Bottleneck-290         [-1, 1024, 14, 14]               0\n          Conv2d-291          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-292          [-1, 256, 14, 14]             512\n            ReLU-293          [-1, 256, 14, 14]               0\n          Conv2d-294          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-295          [-1, 256, 14, 14]             512\n            ReLU-296          [-1, 256, 14, 14]               0\n          Conv2d-297         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n            ReLU-299         [-1, 1024, 14, 14]               0\n      Bottleneck-300         [-1, 1024, 14, 14]               0\n          Conv2d-301          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-302          [-1, 256, 14, 14]             512\n            ReLU-303          [-1, 256, 14, 14]               0\n          Conv2d-304          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-305          [-1, 256, 14, 14]             512\n            ReLU-306          [-1, 256, 14, 14]               0\n          Conv2d-307         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n            ReLU-309         [-1, 1024, 14, 14]               0\n      Bottleneck-310         [-1, 1024, 14, 14]               0\n          Conv2d-311          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-312          [-1, 256, 14, 14]             512\n            ReLU-313          [-1, 256, 14, 14]               0\n          Conv2d-314          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-315          [-1, 256, 14, 14]             512\n            ReLU-316          [-1, 256, 14, 14]               0\n          Conv2d-317         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-318         [-1, 1024, 14, 14]           2,048\n            ReLU-319         [-1, 1024, 14, 14]               0\n      Bottleneck-320         [-1, 1024, 14, 14]               0\n          Conv2d-321          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-322          [-1, 256, 14, 14]             512\n            ReLU-323          [-1, 256, 14, 14]               0\n          Conv2d-324          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-325          [-1, 256, 14, 14]             512\n            ReLU-326          [-1, 256, 14, 14]               0\n          Conv2d-327         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-328         [-1, 1024, 14, 14]           2,048\n            ReLU-329         [-1, 1024, 14, 14]               0\n      Bottleneck-330         [-1, 1024, 14, 14]               0\n          Conv2d-331          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-332          [-1, 256, 14, 14]             512\n            ReLU-333          [-1, 256, 14, 14]               0\n          Conv2d-334          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-335          [-1, 256, 14, 14]             512\n            ReLU-336          [-1, 256, 14, 14]               0\n          Conv2d-337         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-338         [-1, 1024, 14, 14]           2,048\n            ReLU-339         [-1, 1024, 14, 14]               0\n      Bottleneck-340         [-1, 1024, 14, 14]               0\n          Conv2d-341          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-342          [-1, 256, 14, 14]             512\n            ReLU-343          [-1, 256, 14, 14]               0\n          Conv2d-344          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-345          [-1, 256, 14, 14]             512\n            ReLU-346          [-1, 256, 14, 14]               0\n          Conv2d-347         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-348         [-1, 1024, 14, 14]           2,048\n            ReLU-349         [-1, 1024, 14, 14]               0\n      Bottleneck-350         [-1, 1024, 14, 14]               0\n          Conv2d-351          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-352          [-1, 256, 14, 14]             512\n            ReLU-353          [-1, 256, 14, 14]               0\n          Conv2d-354          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-355          [-1, 256, 14, 14]             512\n            ReLU-356          [-1, 256, 14, 14]               0\n          Conv2d-357         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-358         [-1, 1024, 14, 14]           2,048\n            ReLU-359         [-1, 1024, 14, 14]               0\n      Bottleneck-360         [-1, 1024, 14, 14]               0\n          Conv2d-361          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-362          [-1, 256, 14, 14]             512\n            ReLU-363          [-1, 256, 14, 14]               0\n          Conv2d-364          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-365          [-1, 256, 14, 14]             512\n            ReLU-366          [-1, 256, 14, 14]               0\n          Conv2d-367         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-368         [-1, 1024, 14, 14]           2,048\n            ReLU-369         [-1, 1024, 14, 14]               0\n      Bottleneck-370         [-1, 1024, 14, 14]               0\n          Conv2d-371          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-372          [-1, 256, 14, 14]             512\n            ReLU-373          [-1, 256, 14, 14]               0\n          Conv2d-374          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-375          [-1, 256, 14, 14]             512\n            ReLU-376          [-1, 256, 14, 14]               0\n          Conv2d-377         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-378         [-1, 1024, 14, 14]           2,048\n            ReLU-379         [-1, 1024, 14, 14]               0\n      Bottleneck-380         [-1, 1024, 14, 14]               0\n          Conv2d-381          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-382          [-1, 256, 14, 14]             512\n            ReLU-383          [-1, 256, 14, 14]               0\n          Conv2d-384          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-385          [-1, 256, 14, 14]             512\n            ReLU-386          [-1, 256, 14, 14]               0\n          Conv2d-387         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-388         [-1, 1024, 14, 14]           2,048\n            ReLU-389         [-1, 1024, 14, 14]               0\n      Bottleneck-390         [-1, 1024, 14, 14]               0\n          Conv2d-391          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-392          [-1, 256, 14, 14]             512\n            ReLU-393          [-1, 256, 14, 14]               0\n          Conv2d-394          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-395          [-1, 256, 14, 14]             512\n            ReLU-396          [-1, 256, 14, 14]               0\n          Conv2d-397         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-398         [-1, 1024, 14, 14]           2,048\n            ReLU-399         [-1, 1024, 14, 14]               0\n      Bottleneck-400         [-1, 1024, 14, 14]               0\n          Conv2d-401          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-402          [-1, 256, 14, 14]             512\n            ReLU-403          [-1, 256, 14, 14]               0\n          Conv2d-404          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-405          [-1, 256, 14, 14]             512\n            ReLU-406          [-1, 256, 14, 14]               0\n          Conv2d-407         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-408         [-1, 1024, 14, 14]           2,048\n            ReLU-409         [-1, 1024, 14, 14]               0\n      Bottleneck-410         [-1, 1024, 14, 14]               0\n          Conv2d-411          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-412          [-1, 256, 14, 14]             512\n            ReLU-413          [-1, 256, 14, 14]               0\n          Conv2d-414          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-415          [-1, 256, 14, 14]             512\n            ReLU-416          [-1, 256, 14, 14]               0\n          Conv2d-417         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-418         [-1, 1024, 14, 14]           2,048\n            ReLU-419         [-1, 1024, 14, 14]               0\n      Bottleneck-420         [-1, 1024, 14, 14]               0\n          Conv2d-421          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-422          [-1, 256, 14, 14]             512\n            ReLU-423          [-1, 256, 14, 14]               0\n          Conv2d-424          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-425          [-1, 256, 14, 14]             512\n            ReLU-426          [-1, 256, 14, 14]               0\n          Conv2d-427         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-428         [-1, 1024, 14, 14]           2,048\n            ReLU-429         [-1, 1024, 14, 14]               0\n      Bottleneck-430         [-1, 1024, 14, 14]               0\n          Conv2d-431          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-432          [-1, 256, 14, 14]             512\n            ReLU-433          [-1, 256, 14, 14]               0\n          Conv2d-434          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-435          [-1, 256, 14, 14]             512\n            ReLU-436          [-1, 256, 14, 14]               0\n          Conv2d-437         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-438         [-1, 1024, 14, 14]           2,048\n            ReLU-439         [-1, 1024, 14, 14]               0\n      Bottleneck-440         [-1, 1024, 14, 14]               0\n          Conv2d-441          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-442          [-1, 256, 14, 14]             512\n            ReLU-443          [-1, 256, 14, 14]               0\n          Conv2d-444          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-445          [-1, 256, 14, 14]             512\n            ReLU-446          [-1, 256, 14, 14]               0\n          Conv2d-447         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-448         [-1, 1024, 14, 14]           2,048\n            ReLU-449         [-1, 1024, 14, 14]               0\n      Bottleneck-450         [-1, 1024, 14, 14]               0\n          Conv2d-451          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-452          [-1, 256, 14, 14]             512\n            ReLU-453          [-1, 256, 14, 14]               0\n          Conv2d-454          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-455          [-1, 256, 14, 14]             512\n            ReLU-456          [-1, 256, 14, 14]               0\n          Conv2d-457         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-458         [-1, 1024, 14, 14]           2,048\n            ReLU-459         [-1, 1024, 14, 14]               0\n      Bottleneck-460         [-1, 1024, 14, 14]               0\n          Conv2d-461          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-462          [-1, 256, 14, 14]             512\n            ReLU-463          [-1, 256, 14, 14]               0\n          Conv2d-464          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-465          [-1, 256, 14, 14]             512\n            ReLU-466          [-1, 256, 14, 14]               0\n          Conv2d-467         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-468         [-1, 1024, 14, 14]           2,048\n            ReLU-469         [-1, 1024, 14, 14]               0\n      Bottleneck-470         [-1, 1024, 14, 14]               0\n          Conv2d-471          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-472          [-1, 256, 14, 14]             512\n            ReLU-473          [-1, 256, 14, 14]               0\n          Conv2d-474          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-475          [-1, 256, 14, 14]             512\n            ReLU-476          [-1, 256, 14, 14]               0\n          Conv2d-477         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-478         [-1, 1024, 14, 14]           2,048\n            ReLU-479         [-1, 1024, 14, 14]               0\n      Bottleneck-480         [-1, 1024, 14, 14]               0\n          Conv2d-481          [-1, 512, 14, 14]         524,288\n     BatchNorm2d-482          [-1, 512, 14, 14]           1,024\n            ReLU-483          [-1, 512, 14, 14]               0\n          Conv2d-484            [-1, 512, 7, 7]       2,359,296\n     BatchNorm2d-485            [-1, 512, 7, 7]           1,024\n            ReLU-486            [-1, 512, 7, 7]               0\n          Conv2d-487           [-1, 2048, 7, 7]       1,048,576\n     BatchNorm2d-488           [-1, 2048, 7, 7]           4,096\n          Conv2d-489           [-1, 2048, 7, 7]       2,097,152\n     BatchNorm2d-490           [-1, 2048, 7, 7]           4,096\n            ReLU-491           [-1, 2048, 7, 7]               0\n      Bottleneck-492           [-1, 2048, 7, 7]               0\n          Conv2d-493            [-1, 512, 7, 7]       1,048,576\n     BatchNorm2d-494            [-1, 512, 7, 7]           1,024\n            ReLU-495            [-1, 512, 7, 7]               0\n          Conv2d-496            [-1, 512, 7, 7]       2,359,296\n     BatchNorm2d-497            [-1, 512, 7, 7]           1,024\n            ReLU-498            [-1, 512, 7, 7]               0\n          Conv2d-499           [-1, 2048, 7, 7]       1,048,576\n     BatchNorm2d-500           [-1, 2048, 7, 7]           4,096\n            ReLU-501           [-1, 2048, 7, 7]               0\n      Bottleneck-502           [-1, 2048, 7, 7]               0\n          Conv2d-503            [-1, 512, 7, 7]       1,048,576\n     BatchNorm2d-504            [-1, 512, 7, 7]           1,024\n            ReLU-505            [-1, 512, 7, 7]               0\n          Conv2d-506            [-1, 512, 7, 7]       2,359,296\n     BatchNorm2d-507            [-1, 512, 7, 7]           1,024\n            ReLU-508            [-1, 512, 7, 7]               0\n          Conv2d-509           [-1, 2048, 7, 7]       1,048,576\n     BatchNorm2d-510           [-1, 2048, 7, 7]           4,096\n            ReLU-511           [-1, 2048, 7, 7]               0\n      Bottleneck-512           [-1, 2048, 7, 7]               0\nAdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n          Linear-514                 [-1, 1000]       2,049,000\n================================================================\nTotal params: 60,192,808\nTrainable params: 60,192,808\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 606.59\nParams size (MB): 229.62\nEstimated Total Size (MB): 836.78\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = torchvision.models.resnet152().to(device)\n",
    "try:\n",
    "    from torchsummary import summary\n",
    "except Exception as error:\n",
    "    !pip install {str(error)[17:-1]}\n",
    "except:\n",
    "    print(\"Pacote não encontrado\")\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_free_memory():\n",
    "    import GPUtil\n",
    "    CUDA_VISIBLE_DEVICES = os.environ.get('CUDA_VISIBLE_DEVICES')\n",
    "    memory = 0\n",
    "    for GPU in GPUtil.getGPUs():\n",
    "        if CUDA_VISIBLE_DEVICES is None or str(GPU.id) in CUDA_VISIBLE_DEVICES: \n",
    "            memory += GPU.memoryFree\n",
    "    return memory\n",
    "\n",
    "\n",
    "def no_free_mem( mem_per_sample, available ):\n",
    "    return 5*np.array(mem_per_sample).max() > available\n",
    "\n",
    "def main():\n",
    "    model = torchvision.models.resnet152()\n",
    "    model = model.cuda()\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    max_len = 60000\n",
    "\n",
    "    x = get_first_sample(X, max_len, model.inputs_cfg)\n",
    "    y = get_first_sample(Y, max_len, model.outputs_cfg, output=True)\n",
    "    \n",
    "    optimizer = optim.Adam(filter(lambda param: param.requires_grad, model.parameters()))\n",
    "\n",
    "    moremem = True\n",
    "    batch_size = 1\n",
    "    prev_freemem = get_free_memory()\n",
    "    mem_per_sample = [0]\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    while moremem:\n",
    "        y_pred, _, _ = model(x)\n",
    "        freemem = get_free_memory()\n",
    "        if no_free_mem( mem_per_sample, freemem ): break\n",
    "        \n",
    "        loss, _ = model.loss(y_pred, y)\n",
    "        \n",
    "        freemem = min(freemem, get_free_memory())\n",
    "        if no_free_mem( mem_per_sample, freemem ): break\n",
    "\n",
    "        loss.backward()\n",
    "        freemem = min(freemem, get_free_memory())\n",
    "        if no_free_mem( mem_per_sample, freemem ): break\n",
    "\n",
    "        optimizer.step()\n",
    "        freemem = min(freemem, get_free_memory())\n",
    "        if no_free_mem( mem_per_sample, freemem ): break\n",
    "\n",
    "        if prev_freemem - freemem > 0:\n",
    "            mem_per_sample.append(prev_freemem - freemem)\n",
    "\n",
    "        if no_free_mem( mem_per_sample, freemem ): break\n",
    "    \n",
    "        batch_size += 1\n",
    "        prev_freemem = min(prev_freemem, freemem)\n",
    "\n",
    "        x = insert_sample(x)\n",
    "        y = insert_sample(y)\n",
    "        \n",
    "    print(\"GUESSING batch_size, \", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'get_first_sample' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-45158ce47a56>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_first_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_first_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_first_sample' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pytorch_lightning\n",
      "  Downloading pytorch_lightning-1.2.4-py3-none-any.whl (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 1.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/lib/python3/dist-packages (from pytorch_lightning) (5.3.1)\n",
      "Collecting future>=0.17.1\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 8.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /home/matheuscosta/.local/lib/python3.8/site-packages (from pytorch_lightning) (4.57.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/matheuscosta/.local/lib/python3.8/site-packages (from pytorch_lightning) (2.4.1)\n",
      "Requirement already satisfied: torch>=1.4 in /home/matheuscosta/.local/lib/python3.8/site-packages (from pytorch_lightning) (1.7.1+cu110)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/matheuscosta/.local/lib/python3.8/site-packages (from pytorch_lightning) (1.19.5)\n",
      "Collecting fsspec[http]>=0.8.1\n",
      "  Downloading fsspec-0.8.7-py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 11.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/matheuscosta/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/matheuscosta/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/matheuscosta/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.32.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/matheuscosta/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/matheuscosta/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/lib/python3/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.12.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/matheuscosta/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (49.3.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/matheuscosta/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.28.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/matheuscosta/.local/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/matheuscosta/.local/lib/python3.8/site-packages (from torch>=1.4->pytorch_lightning) (3.7.4.3)\n",
      "Collecting aiohttp; extra == \"http\"\n",
      "  Downloading aiohttp-3.7.4.post0-cp38-cp38-manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 9.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /home/matheuscosta/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/matheuscosta/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/matheuscosta/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/matheuscosta/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 13.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: chardet<5.0,>=2.0 in /usr/lib/python3/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/matheuscosta/.local/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (20.3.0)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[K     |████████████████████████████████| 324 kB 12.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/matheuscosta/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (2.10)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=a28ea53bee8d965dfc2c61007944d276eab67b4a7b4ccc54f50ed6b4b40f55f9\n",
      "  Stored in directory: /home/matheuscosta/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "Successfully built future\n",
      "\u001b[31mERROR: awsebcli 3.19.3 has requirement future<0.17.0,>=0.16.0, but you'll have future 0.18.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: future, multidict, async-timeout, yarl, aiohttp, fsspec, pytorch-lightning\n",
      "  Attempting uninstall: future\n",
      "    Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-0.8.7 future-0.18.2 multidict-5.1.0 pytorch-lightning-1.2.4 yarl-1.6.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    from pytorch_lightning import Trainer\n",
    "except Exception as error:\n",
    "    !pip install {str(error)[17:-1]}\n",
    "except:\n",
    "    print(\"Pacote não encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleAttributeError",
     "evalue": "'ResNet' object has no attribute 'automatic_optimization'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-330248f68adc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet152\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_scale_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binsearch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtune\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \"\"\"\n\u001b[0;32m-> 1054\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_setup_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\u001b[0m in \u001b[0;36mtune\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_scale_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_scale_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'power'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             self.scale_batch_size(\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_scale_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\u001b[0m in \u001b[0;36mscale_batch_size\u001b[0;34m(self, model, mode, steps_per_trial, init_val, max_trials, batch_arg_name, **fit_kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         return scale_batch_size(\n\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/tuner/tuning.py\u001b[0m in \u001b[0;36msetup_trainer\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mdatamodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLightningDataModule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     ):\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_trainer_model_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;31m# setup data, etc...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/model_connector.py\u001b[0m in \u001b[0;36mcopy_trainer_model_properties\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mref_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mautomatic_optimization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mref_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    779\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'ResNet' object has no attribute 'automatic_optimization'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = torchvision.models.resnet152().to(device)\n",
    "trainer = Trainer(auto_scale_batch_size='binsearch')\n",
    "trainer.tune(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}